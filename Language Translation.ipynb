{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac47ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import torchtext\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from collections import Counter\n",
    "# from torchtext.vocab import Vocab\n",
    "# from torchtext.utils import download_from_url,extract_archive\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import Tensor\n",
    "# from torch.nn import (TransformerEncoder,TransformerDecoder,TransformerEncoderLayer,TransformerDecoderLayer)\n",
    "# import io\n",
    "# import time\n",
    "# #https://www.analyticsvidhya.com/blog/2021/06/language-translation-with-transformer-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45122156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# import torch\n",
    "# from collections import Counter\n",
    "# from torchtext.vocab import Vocab\n",
    "# from torchtext.utils import download_from_url, extract_archive\n",
    "# from spacy.lang.de import German\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "# url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "# train_urls = ('train.de.gz', 'train.en.gz')\n",
    "# val_urls = ('val.de.gz', 'val.en.gz')\n",
    "# test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "# train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "# val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "# test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "# de_tokenizer = German().tokenizer\n",
    "# en_tokenizer = English().tokenizer\n",
    "\n",
    "# def build_vocab(filepath, tokenizer):\n",
    "#     counter = Counter()\n",
    "#     with io.open(filepath, encoding='utf8') as f:\n",
    "#         for string_ in f:\n",
    "#             counter.update(tokenizer(string_))\n",
    "#     return Vocab(counter)\n",
    "\n",
    "# de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "# en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "# def data_process(filepaths, de_vocab, en_vocab, de_tokenizer, en_tokenizer):\n",
    "#     print('Processing files:', filepaths)\n",
    "#     raw_de_iter = iter(io.open(filepaths[0], encoding='utf8'))\n",
    "#     raw_en_iter = iter(io.open(filepaths[1], encoding='utf8'))\n",
    "#     data = []\n",
    "#     for raw_de, raw_en in zip(raw_de_iter, raw_en_iter):\n",
    "#         de_tensor = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de.rstrip('\\n'))], dtype=torch.long)\n",
    "#         en_tensor = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip('\\n'))], dtype=torch.long)\n",
    "#         if len(de_tensor) == 0 or len(en_tensor) == 0:\n",
    "#             print('Warning: Empty tensor found!')\n",
    "#             continue\n",
    "#         data.append((de_tensor, en_tensor))\n",
    "#     print('Number of samples:', len(data))\n",
    "#     print(data)\n",
    "#     return data\n",
    "\n",
    "# train_data = data_process(train_filepaths, de_vocab, en_vocab, de_tokenizer, en_tokenizer)\n",
    "# val_data = data_process(val_filepaths, de_vocab, en_vocab, de_tokenizer, en_tokenizer)\n",
    "# test_data = data_process(test_filepaths, de_vocab, en_vocab, de_tokenizer, en_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca49e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd268f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n",
    "#This code appears to be a method of a Python class that is responsible for building a vocabulary of words. It takes a word as input and updates several attributes of the class to incorporate the new word into the vocabulary.\n",
    "\n",
    "#The addWord method first checks if the word is already present in the word2index dictionary. If the word is not in the dictionary, it means that the word is new and needs to be added to the vocabulary. In this case, the method adds the word to the word2index dictionary and assigns it a new index that corresponds to the current size of the vocabulary. The method then initializes the word2count dictionary for this word, setting the count to 1, since this is the first time this word has been seen. Finally, the method adds the word to the index2word dictionary, mapping the index to the word, and increments the total number of words in the vocabulary, n_words, by 1.\n",
    "\n",
    "#On the other hand, if the word is already in the word2index dictionary, it means that the word has already been added to the vocabulary before. In this case, the method simply increments the count of the word in the word2count dictionary.\n",
    "\n",
    "#Overall, this method is used as part of a larger process for building a vocabulary of words, which is often an important preprocessing step in natural language processing tasks such as text classification, machine translation, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c611daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    ) #Removing diactirc symbols (Â´), grave accents (`), circumflex accents (^),\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692bde5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "#This is a function called readLangs that reads and processes data from a file containing parallel text in two languages, lang1 and lang2. The function returns two Lang instances representing the input and output languages, respectively, as well as a list of sentence pairs, where each pair consists of a sentence in lang1 and its corresponding translation in lang2.\n",
    "\n",
    "#The function begins by opening the file located in the data directory with the name lang1-lang2.txt, where lang1 and lang2 are the input parameters. The file is assumed to contain one sentence pair per line, separated by a tab character.\n",
    "\n",
    "#The read() method is called to read the contents of the file as a single string, and the strip() method is called to remove any leading or trailing whitespace characters. The resulting string is then split into a list of lines using the split('\\n') method.\n",
    "\n",
    "#The pairs variable is initialized as a list comprehension that iterates over the lines in the file and splits each line into a pair of sentences using the split('\\t') method. The normalizeString function is called on each sentence in the pair to remove any unwanted characters or diacritics, and the resulting normalized sentences are stored as a new pair in the pairs list.\n",
    "\n",
    "#If the reverse parameter is set to True, the order of the sentence pairs is reversed, so that the translation in lang2 comes first and the original sentence in lang1 comes second. In this case, the pairs list is modified by reversing the order of the sentences in each pair using the reversed() function.\n",
    "\n",
    "#Two Lang instances are then created to represent the input and output languages, respectively, depending on the value of the reverse parameter. If reverse is True, the Lang instance for the input language is created with the name lang2, and the Lang instance for the output language is created with the name lang1. Otherwise, the input and output languages are created with the names lang1 and lang2, respectively.\n",
    "\n",
    "#Finally, the function returns the two Lang instances and the list of sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d0088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "#This is a Python code that defines two functions: filterPair and filterPairs.\n",
    "\n",
    "#The filterPair function takes a pair of sentences (as a tuple) as input and returns True if the length of the first sentence (the source sentence) and the second sentence (the target sentence) are both less than MAX_LENGTH, which is set to 10 in this code, and if the second sentence starts with any of the English prefixes defined in eng_prefixes.\n",
    "\n",
    "#The filterPairs function takes a list of pairs of sentences as input and returns a new list of pairs that meet the conditions defined in filterPair. It uses a list comprehension to iterate over each pair in the input list and filter out any pairs that do not meet the conditions.\n",
    "\n",
    "#Overall, these functions are used to filter out pairs of sentences that are too long or do not match the expected format, based on the English prefixes defined in the eng_prefixes tuple. These filtered pairs can be used for various natural language processing tasks, such as machine translation or language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7ee9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['nous sommes en train de lire .', 'we re reading .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17126887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "#This is a PyTorch implementation of an Encoder RNN (Recurrent Neural Network) class. The Encoder RNN is a type of neural network used in natural language processing and machine translation tasks to encode a sequence of words into a fixed-length vector representation.\n",
    "\n",
    "#The __init__ method initializes the Encoder RNN class and defines its components. The input_size parameter is the size of the input vocabulary (i.e., the number of unique words in the input language), and hidden_size is the size of the hidden state of the RNN.\n",
    "\n",
    "#The embedding layer is used to map the input words to a continuous vector space of size hidden_size. This is necessary because the RNN cannot work with discrete inputs. The gru layer is a gated recurrent unit that processes the input sequence and updates its hidden state at each time step.\n",
    "\n",
    "#The forward method defines the forward pass of the Encoder RNN. It takes as input an input tensor representing a sequence of words and a hidden tensor representing the initial hidden state of the RNN. The input is first passed through the embedding layer to obtain a continuous vector representation. The resulting vector is then passed through the gru layer, which updates the hidden state of the RNN and produces an output tensor. The output tensor and the updated hidden state are returned.\n",
    "\n",
    "#The initHidden method initializes the hidden state of the RNN with zeros. It returns a tensor of shape (1, 1, hidden_size) representing the initial hidden state.\n",
    "\n",
    "#n a Recurrent Neural Network (RNN), the hidden state is the network's internal state that contains information about the previous inputs processed by the network. At each time step, the RNN takes an input and updates its hidden state based on the current input and the previous hidden state. \n",
    "#The hidden state can be seen as a compressed representation of the past inputs that the network has processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c911db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "#This code defines a class called DecoderRNN that inherits from nn.Module. This class is used for implementing a recurrent neural network (RNN) for decoding a sequence of output tokens.\n",
    "\n",
    "#The __init__ method initializes the parameters for the decoder RNN. The hidden_size is the number of features in the hidden state of the RNN, and output_size is the number of possible output tokens.\n",
    "\n",
    "#The decoder RNN has four layers:\n",
    "\n",
    "#nn.Embedding(output_size, hidden_size) initializes an embedding layer with output_size inputs and hidden_size outputs. It is used to convert the input tokens into a continuous vector space.\n",
    "\n",
    "#nn.GRU(hidden_size, hidden_size) initializes a GRU layer with hidden_size inputs and hidden_size outputs. It is the core layer of the decoder RNN and is responsible for computing the hidden state.\n",
    "\n",
    "#nn.Linear(hidden_size, output_size) initializes a linear layer with hidden_size inputs and output_size outputs. It is used to convert the hidden state of the RNN to a probability distribution over the output tokens.\n",
    "\n",
    "#nn.LogSoftmax(dim=1) initializes a log-softmax layer that takes the output of the linear layer and normalizes it to a probability distribution.\n",
    "\n",
    "#The forward method takes two inputs, an input token and the previous hidden state of the RNN, and returns the output token and the updated hidden state.\n",
    "\n",
    "#The input token is first passed through the embedding layer, then passed through a ReLU activation function. The resulting output is then passed through the GRU layer along with the previous hidden state. The resulting output from the GRU layer is passed through the linear layer and then through the log-softmax layer to get the probability distribution over the output tokens. The output token with the highest probability is returned as the predicted token for the current step.\n",
    "\n",
    "#The initHidden method initializes the hidden state to a tensor of zeros with the shape (1, 1, hidden_size) and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594c6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "#This code defines a decoder RNN with attention mechanism. The attention mechanism is used to help the decoder focus on different parts of the input sequence during decoding. This is particularly useful when dealing with long input sequences.\n",
    "\n",
    "#The AttnDecoderRNN class takes as input the hidden_size, output_size, dropout_p, and max_length hyperparameters. The hidden_size is the size of the hidden state of the decoder, output_size is the size of the output vocabulary, dropout_p is the probability of dropout, and max_length is the maximum length of the input sequence.\n",
    "\n",
    "#In the constructor, the class initializes several layers that are used in the forward method. These layers include an embedding layer, an attention layer, an attention-combination layer, a dropout layer, a GRU layer, and an output layer.\n",
    "\n",
    "#The forward method takes as input the current token input, the previous hidden state hidden, and the encoder outputs encoder_outputs.\n",
    "\n",
    "#First, the input token is passed through the embedding layer and then through a dropout layer. The dropout layer randomly zeroes some of the elements of the input tensor to prevent overfitting.\n",
    "\n",
    "#Next, the attention mechanism is applied to the embedded input and the previous hidden state. The attention layer computes a weight for each encoder output based on how well it matches the input and hidden state. The attention-combination layer then combines the attention weights with the encoder outputs to produce a context vector.\n",
    "\n",
    "#The context vector and the embedded input are concatenated together and passed through the GRU layer to compute the new hidden state. Finally, the output of the GRU layer is passed through a linear layer and a log-softmax layer to produce the output probabilities.\n",
    "\n",
    "#The initHidden method initializes the hidden state of the decoder RNN with zeros.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9488a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4379280",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d55969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ed0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62a0d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed200e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c57b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f0aff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 0s (- 70m 10s) (5000 6%) 2.8300\n",
      "9m 57s (- 64m 43s) (10000 13%) 2.2585\n",
      "15m 1s (- 60m 4s) (15000 20%) 1.9680\n",
      "20m 1s (- 55m 4s) (20000 26%) 1.7034\n",
      "25m 1s (- 50m 3s) (25000 33%) 1.5601\n",
      "29m 59s (- 44m 59s) (30000 40%) 1.3777\n",
      "35m 0s (- 40m 0s) (35000 46%) 1.2465\n",
      "40m 1s (- 35m 0s) (40000 53%) 1.1151\n",
      "56m 26s (- 37m 37s) (45000 60%) 1.0188\n",
      "61m 28s (- 30m 44s) (50000 66%) 0.9150\n",
      "66m 33s (- 24m 12s) (55000 73%) 0.8246\n",
      "71m 34s (- 17m 53s) (60000 80%) 0.7638\n",
      "76m 41s (- 11m 47s) (65000 86%) 0.7070\n",
      "81m 41s (- 5m 50s) (70000 93%) 0.6395\n",
      "86m 41s (- 0m 0s) (75000 100%) 0.5885\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54366ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> nous sommes a court de biere .\n",
      "= we re out of beer .\n",
      "< we re out of . . <EOS>\n",
      "\n",
      "> vous etes incroyablement talentueux .\n",
      "= you re incredibly talented .\n",
      "< you re incredibly talented . <EOS>\n",
      "\n",
      "> nous ne sommes pas en securite .\n",
      "= we re not safe .\n",
      "< we re not safe safe . <EOS>\n",
      "\n",
      "> je suis desolee si je vous ai embarrassee .\n",
      "= i m sorry if i embarrassed you .\n",
      "< i m sorry if i embarrassed you . <EOS>\n",
      "\n",
      "> tu es une opportuniste .\n",
      "= you re opportunistic .\n",
      "< you re opportunistic . <EOS>\n",
      "\n",
      "> je suis libre aujourd hui .\n",
      "= i m free today .\n",
      "< i m free today . <EOS>\n",
      "\n",
      "> je perds patience avec toi .\n",
      "= i am losing my patience with you .\n",
      "< i am losing my patience with you . <EOS>\n",
      "\n",
      "> je lis une lettre .\n",
      "= i am reading a letter .\n",
      "< i am reading a letter . <EOS>\n",
      "\n",
      "> tu en fais partie .\n",
      "= you re part of this .\n",
      "< you re part of them . <EOS>\n",
      "\n",
      "> vous etes l elue .\n",
      "= you are the chosen one .\n",
      "< you are the chosen one . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a365193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2962f669e90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7228c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = elle a cinq ans de moins que moi .\n",
      "output = she is five years younger than me . <EOS>\n",
      "input = elle est trop petit .\n",
      "output = she s too short . <EOS>\n",
      "input = je ne crains pas de mourir .\n",
      "output = i m not scared to die . <EOS>\n",
      "input = c est un jeune directeur plein de talent\n",
      "output = he s a talented young director . <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malhotralovish\\AppData\\Local\\Temp\\ipykernel_956\\522407878.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "C:\\Users\\malhotralovish\\AppData\\Local\\Temp\\ipykernel_956\\522407878.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\malhotralovish\\AppData\\Local\\Temp\\ipykernel_956\\522407878.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = je suis ravi de vous rencontrer\n",
      "output = i m delighted to meet you . <EOS>\n",
      "input = sois un homme\n",
      "output = i m a . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent\")\n",
    "evaluateAndShowAttention(\"je suis ravi de vous rencontrer\")\n",
    "evaluateAndShowAttention(\"sois un homme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc7e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae55e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1f707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c967b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
